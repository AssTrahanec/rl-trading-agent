# Experiment Log

–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å RL Trading Agent.

---

## Experiment Run #1 (Baseline)

**–î–∞—Ç–∞:** 2025-01-21
**–¶–µ–ª—å:** –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å baseline –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å —Ä–∞–∑–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ reward –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
**–î–∞–Ω–Ω—ã–µ:** –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ (500 –¥–Ω–µ–π, seed=42)
**–û–±—É—á–µ–Ω–∏–µ:** PPO, 50K timesteps (5 epochs √ó 10K)

### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

| Experiment | Reward Type | Normalize Obs | Entropy | Transaction Penalty |
|------------|-------------|---------------|---------|---------------------|
| baseline | simple_pnl | False | 0.01 | 0.0 |
| improved_reward | sharpe_based | False | 0.01 | 0.001 |
| normalized_obs | simple_pnl | True | 0.01 | 0.0 |
| combo | sharpe_based | True | 0.02 | 0.001 |

### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

| Experiment | Return | vs Buy&Hold | Sharpe | Drawdown | Win Rate | Trades |
|------------|--------|-------------|--------|----------|----------|--------|
| **normalized_obs** | +123.65% | **+52.22%** | 2.35 | **-8.66%** | 30.51% | 152 |
| baseline | +108.13% | +36.70% | 1.87 | -11.24% | 40.53% | 298 |
| combo | +101.17% | +29.74% | **2.40** | -12.75% | **66.15%** | 139 |
| improved_reward | +73.05% | +1.62% | 1.37 | -12.79% | 51.67% | 309 |

**Buy & Hold baseline:** +71.43%

### –ê–Ω–∞–ª–∏–∑

#### –ß—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:
1. **–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è observations** - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞! –ë–µ–∑ –Ω–µ—ë sharpe_based reward –ø–æ—á—Ç–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
2. **Simple PnL + Normalized** - –ª—É—á—à–∏–π return (+123.65%) –∏ –ª—É—á—à–∏–π drawdown (-8.66%)
3. **Combo** - –ª—É—á—à–∏–π Sharpe (2.40) –∏ win rate (66.15%), —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å

#### –ß—Ç–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç:
1. **Sharpe-based –±–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏** - outperformance –≤—Å–µ–≥–æ +1.62%, —Ö—É–∂–µ baseline
2. Transaction penalty –±–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –º–µ—à–∞–µ—Ç –æ–±—É—á–µ–Ω–∏—é

#### Action Distribution –ø–∞—Ç—Ç–µ—Ä–Ω—ã:
- **normalized_obs**: –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è - 46% SELL_100%, 33% BUY_50%, –ø–æ—á—Ç–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç HOLD
- **combo**: –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è - 17.8% HOLD, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ buy/sell
- **baseline**: –ú–Ω–æ–≥–æ –º–µ–ª–∫–∏—Ö —Å–¥–µ–ª–æ–∫ (298), –∞–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç SELL_25%

### –í—ã–≤–æ–¥—ã –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤:

1. –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `normalize_obs=True`
2. –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —É–≤–µ–ª–∏—á–∏—Ç—å timesteps (100K+) –¥–ª—è –ª—É—á—à–µ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
3. –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ learning rates
4. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –≤–ª–∏—è–Ω–∏–µ entropy coefficient –Ω–∞ exploration
5. –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ transaction penalties —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π

---

## Experiment Run #2 (Optimization)

**–î–∞—Ç–∞:** 2025-01-21
**–¶–µ–ª—å:** –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã–≤–æ–¥–æ–≤ Run #1
**–ì–∏–ø–æ—Ç–µ–∑—ã:**
1. –ë–æ–ª—å—à–µ timesteps ‚Üí –ª—É—á—à–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å
2. –ú–µ–Ω—å—à–∏–π learning rate ‚Üí –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
3. –í—ã—à–µ entropy ‚Üí –±–æ–ª—å—à–µ exploration ‚Üí –≤–æ–∑–º–æ–∂–Ω–æ –ª—É—á—à–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
4. Transaction penalty —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å Sharpe

### –ù–æ–≤—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

| Experiment | –ë–∞–∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞ | –ò–∑–º–µ–Ω–µ–Ω–∏—è |
|------------|---------------|-----------|
| norm_100k | normalized_obs | timesteps: 50K ‚Üí 100K |
| norm_low_lr | normalized_obs | learning_rate: 3e-4 ‚Üí 1e-4 |
| norm_high_ent | normalized_obs | entropy_coef: 0.01 ‚Üí 0.05 |
| norm_with_penalty | normalized_obs | +transaction_penalty: 0.002 |

### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

| Experiment | Return | vs Buy&Hold | Sharpe | Drawdown | Win Rate | Trades |
|------------|--------|-------------|--------|----------|----------|--------|
| **norm_100k** | **+187.20%** | **+115.76%** | 2.59 | -9.68% | 35.56% | 222 |
| norm_high_ent | +155.84% | +84.41% | **2.75** | **-7.40%** | **41.87%** | 179 |
| normalized_obs (baseline) | +118.92% | +47.49% | 2.31 | -8.11% | 35.06% | 229 |
| norm_with_penalty | +109.49% | +38.06% | 2.38 | -8.33% | 36.36% | 187 |
| norm_100k_low_lr | +102.23% | +30.80% | 1.89 | -9.88% | 38.54% | 223 |
| norm_low_lr | +89.19% | +17.76% | 1.78 | -11.65% | 35.29% | 229 |

**Buy & Hold baseline:** +71.43%

### –ê–Ω–∞–ª–∏–∑

#### –ö–ª—é—á–µ–≤—ã–µ –Ω–∞—Ö–æ–¥–∫–∏:

1. **–ë–æ–ª—å—à–µ timesteps = –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à–µ** ‚úÖ
   - norm_100k (+187%) vs baseline (+118%) = +69% improvement
   - –ì–∏–ø–æ—Ç–µ–∑–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞

2. **–í—ã—Å–æ–∫–∏–π entropy —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ –Ω–µ return** ‚úÖ
   - norm_high_ent: –ª—É—á—à–∏–π Sharpe (2.75) –∏ –ª—É—á—à–∏–π Win Rate (41.87%)
   - –ù–æ return –Ω–∏–∂–µ —á–µ–º —É norm_100k
   - –ë–æ–ª—å—à–µ exploration ‚Üí –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è

3. **–ù–∏–∑–∫–∏–π learning rate = –•–£–ñ–ï** ‚ùå
   - norm_low_lr (+89%) vs baseline (+118%) = -29% regression
   - norm_100k_low_lr (+102%) vs norm_100k (+187%) = -85% regression
   - –ì–∏–ø–æ—Ç–µ–∑–∞ –æ—Ç–≤–µ—Ä–≥–Ω—É—Ç–∞! –ú–µ–Ω—å—à–∏–π LR –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç

4. **Transaction penalty –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –≤–ª–∏—è–µ—Ç**
   - norm_with_penalty (+109%) –±–ª–∏–∑–∫–æ –∫ baseline (+118%)
   - –ù–µ —É–ª—É—á—à–∞–µ—Ç –∏ –Ω–µ —Å–∏–ª—å–Ω–æ –ø–æ—Ä—Ç–∏—Ç

#### –ß—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:
- ‚úÖ –ë–æ–ª—å—à–µ timesteps (100K >> 50K)
- ‚úÖ –í—ã—Å–æ–∫–∏–π entropy (0.05) –¥–ª—è –ª—É—á—à–µ–≥–æ Sharpe/Win Rate
- ‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π learning rate (3e-4)

#### –ß—Ç–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç:
- ‚ùå –ù–∏–∑–∫–∏–π learning rate (1e-4) - —É—Ö—É–¥—à–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- ‚ö†Ô∏è Transaction penalty - –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç

---

## Experiment Run #3 (Scaling Up)

**–î–∞—Ç–∞:** 2025-01-21
**–¶–µ–ª—å:** –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –ª—É—á—à–∏–µ –Ω–∞—Ö–æ–¥–∫–∏ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å
**–ì–∏–ø–æ—Ç–µ–∑—ã:**
1. 100K + high entropy = –ª—É—á—à–µ–µ –∏–∑ –æ–±–æ–∏—Ö –º–∏—Ä–æ–≤ (return + Sharpe)
2. 200K timesteps = –µ—â–µ –ª—É—á—à–µ —á–µ–º 100K
3. –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π LR (5e-4) –º–æ–∂–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å

### –ù–æ–≤—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

| Experiment | –ë–∞–∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞ | –ò–∑–º–µ–Ω–µ–Ω–∏—è |
|------------|---------------|-----------|
| combo_best | norm_100k | +entropy_coef: 0.05 (100K + high entropy) |
| steps_200k | norm_100k | timesteps: 100K ‚Üí 200K |
| high_lr | normalized_obs | learning_rate: 3e-4 ‚Üí 5e-4 |
| steps_200k_ent | steps_200k | +entropy_coef: 0.05 (–º–∞–∫—Å–∏–º—É–º) |

### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

| Experiment | Return | vs Buy&Hold | Sharpe | Drawdown | Win Rate | Trades |
|------------|--------|-------------|--------|----------|----------|--------|
| high_lr | **+245.71%** | **+174.27%** | 2.97 | -15.10% | **42.76%** | 149 |
| **combo_best** | +232.96% | +161.53% | **3.32** | **-9.66%** | 30.73% | 148 |
| steps_200k_ent | +201.37% | +129.93% | 2.70 | -12.32% | 34.74% | 119 |
| steps_200k | +175.86% | +104.43% | 2.85 | -10.66% | 31.63% | 152 |
| norm_100k (baseline) | +154.04% | +82.61% | 2.71 | -11.25% | 29.40% | 116 |
| norm_high_ent (baseline) | +130.87% | +59.43% | 2.52 | -9.20% | 38.75% | 195 |

**Buy & Hold baseline:** +71.43%

### –ê–Ω–∞–ª–∏–∑

#### –ö–ª—é—á–µ–≤—ã–µ –Ω–∞—Ö–æ–¥–∫–∏:

1. **combo_best (100K + entropy 0.05) = –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å** üèÜ
   - Sharpe 3.32 (–õ–£–ß–®–ò–ô!)
   - Return +232.96%
   - Drawdown -9.66% (–≤ —Ä–∞–º–∫–∞—Ö -15%)
   - –ì–∏–ø–æ—Ç–µ–∑–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –¥–∞–µ—Ç –ª—É—á—à–µ–µ –∏–∑ –æ–±–æ–∏—Ö –º–∏—Ä–æ–≤

2. **high_lr (5e-4) = –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π return, –Ω–æ —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω–æ** ‚ö†Ô∏è
   - Return +245.71% (–õ–£–ß–®–ò–ô!)
   - Win Rate 42.76% (–õ–£–ß–®–ò–ô!)
   - –ù–û Drawdown -15.10% (–Ω–∞ –≥—Ä–∞–Ω–∏ –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ!)
   - –í—ã—Å–æ–∫–∏–π LR —É—Å–∫–æ—Ä—è–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ä–∏—Å–∫

3. **200K timesteps = diminishing returns** üìâ
   - steps_200k (+175%) < combo_best (+232%)
   - steps_200k_ent (+201%) < combo_best (+232%)
   - –ë–æ–ª—å—à–µ —à–∞–≥–æ–≤ –Ω–µ –≤—Å–µ–≥–¥–∞ –ª—É—á—à–µ! 100K + –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π entropy –≤–∞–∂–Ω–µ–µ

4. **–í—ã—Å–æ–∫–∏–π entropy –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–µ–Ω –¥–ª—è Sharpe**
   - combo_best (entropy 0.05): Sharpe 3.32
   - steps_200k (entropy 0.01): Sharpe 2.85
   - –†–∞–∑–Ω–∏—Ü–∞ –æ–≥—Ä–æ–º–Ω–∞—è –ø—Ä–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —à–∞–≥–∞—Ö

#### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:

**WINNER: combo_best** (100K timesteps + entropy 0.05)
- –õ—É—á—à–∏–π risk-adjusted return (Sharpe 3.32)
- –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π drawdown (-9.66%)
- –û—Ç–ª–∏—á–Ω—ã–π outperformance (+161.53%)

---

## üéâ MILESTONE 1 COMPLETE!

**combo_best** –ø—Ä–æ—Ö–æ–¥–∏—Ç –≤—Å–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏:

| –ö—Ä–∏—Ç–µ—Ä–∏–π | –¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ | –†–µ–∑—É–ª—å—Ç–∞—Ç | –°—Ç–∞—Ç—É—Å |
|----------|------------|-----------|--------|
| Outperformance | > 5% | +161.53% | ‚úÖ PASS |
| Sharpe Ratio | > 1.0 | 3.32 | ‚úÖ PASS |
| Max Drawdown | < -15% | -9.66% | ‚úÖ PASS |

**–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥:** Milestone 2 - –†–µ–∞–ª—å–Ω—ã–µ —Ü–µ–Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (yfinance/ccxt)

---

## Summary Table (All Runs)

| Run | Best Experiment | Return | Sharpe | Drawdown | Key Finding |
|-----|-----------------|--------|--------|----------|-------------|
| #1 | normalized_obs | +123.65% | 2.35 | -8.66% | –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏—Ç–∏—á–Ω–∞ |
| #2 | norm_100k | +187.20% | 2.59 | -9.68% | –ë–æ–ª—å—à–µ timesteps = –ª—É—á—à–µ |
| #3 | **combo_best** | **+232.96%** | **3.32** | -9.66% | 100K + entropy 0.05 = –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å |

### –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (Milestone 1 Winner)

```python
{
    "name": "combo_best",
    "reward_type": "simple_pnl",
    "normalize_obs": True,
    "entropy_coef": 0.05,      # –ö–ª—é—á –∫ –≤—ã—Å–æ–∫–æ–º—É Sharpe!
    "transaction_penalty": 0.0,
    "learning_rate": 3e-4,
    "timesteps": 100000,       # 100K –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
}
```

---

## Hyperparameter Reference

### PPO Defaults (—Ç–µ–∫—É—â–∏–µ)
```python
learning_rate = 3e-4
n_steps = 2048
batch_size = 64
n_epochs = 10
gamma = 0.99
gae_lambda = 0.95
clip_range = 0.2
ent_coef = 0.01  # varies by experiment
```

### Training Defaults
```python
total_timesteps = 50000  # 5 epochs √ó 10K
```

### Environment
```python
initial_balance = 10000
commission = 0.001  # 0.1%
observation_space:
  - market: 15 features
  - news: 6 features
  - portfolio: 5 features
action_space: Discrete(7)  # HOLD, BUY 25/50/100%, SELL 25/50/100%
```
