{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RL News Trading Agent - Google Colab Training\n\nSingle-file implementation for iterative development.\n\n## Sections:\n1. **Setup** - Smart install (skips already installed packages)\n2. **Config** - Experiment configurations\n3. **Data** - Generate/load cached market & news data\n4. **Environment** - Custom Gymnasium trading environment\n5. **Training** - Train PPO agent (saves models automatically)\n5b. **Quick Evaluate** - Load saved models WITHOUT retraining ‚ö°\n6. **Results** - Display metrics with CLAUDE_RESULTS markers\n7. **Visualization** - Training progress plots\n\n---\n\n## üöÄ Quick Start\n\n**–ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫:** `Runtime ‚Üí Run All` (~5 min)\n\n**–ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –∑–∞–ø—É—Å–∫–∏ (–±—ã—Å—Ç—Ä–æ):**\n1. Run Section 1-4 (Setup, Config, Data, Environment)\n2. Run Section 5b (Quick Evaluate) ‚Üê –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è\n3. Run Section 6-7 (Results)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# SECTION 1: Setup (Smart Install)\n# ============================================\n# –ó–∞–ø—É—Å–∫–∞—Ç—å 1 —Ä–∞–∑ –≤ –Ω–∞—á–∞–ª–µ —Å–µ—Å—Å–∏–∏\n# –ü—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—É—Å–∫–∞—Ö –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –ø–∞–∫–µ—Ç—ã\n\nimport subprocess\nimport sys\n\ndef install_if_missing(package, import_name=None):\n    \"\"\"Install package only if not already installed.\"\"\"\n    if import_name is None:\n        import_name = package.split('[')[0].replace('-', '_')\n    try:\n        __import__(import_name)\n        return False  # Already installed\n    except ImportError:\n        print(f\"üì¶ Installing {package}...\")\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])\n        return True\n\n# Install only missing packages\npackages = [\n    ('stable-baselines3[extra]', 'stable_baselines3'),\n    ('gymnasium', 'gymnasium'),\n    ('yfinance', 'yfinance'),\n    ('ta', 'ta'),\n    ('plotly', 'plotly'),\n]\n\ninstalled_count = 0\nfor pkg_info in packages:\n    if isinstance(pkg_info, tuple):\n        pkg, imp = pkg_info\n    else:\n        pkg, imp = pkg_info, None\n    if install_if_missing(pkg, imp):\n        installed_count += 1\n\nif installed_count > 0:\n    print(f\"‚úÖ Installed {installed_count} new packages\")\nelse:\n    print(\"‚úÖ All packages already installed (skipped)\")\n\n# Now import everything\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nimport gymnasium as gym\nfrom gymnasium import spaces\nfrom stable_baselines3 import PPO, SAC, A2C\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.callbacks import BaseCallback\nimport traceback\nimport time\n\n# Define paths for caching\nDATA_CACHE_PATH = '/content/data_cache.npz'\nMODELS_DIR = '/content/models'\nRESULTS_PATH = '/content/experiment_results.json'\n\nprint(f\"\\n‚úÖ Setup complete\")\nprint(f\"Gymnasium version: {gym.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"Pandas version: {pd.__version__}\")\nprint(f\"\\nüìÅ Cache paths:\")\nprint(f\"  Data: {DATA_CACHE_PATH}\")\nprint(f\"  Models: {MODELS_DIR}\")\nprint(f\"  Results: {RESULTS_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "source": "---\n## SECTION 2: Experiment Configuration\n\nDefine multiple experiments to compare different training approaches.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# SECTION 2: Experiment Configuration\n# ============================================\n# Run #3: Scaling Up - –∫–æ–º–±–∏–Ω–∏—Ä—É–µ–º –ª—É—á—à–∏–µ –Ω–∞—Ö–æ–¥–∫–∏\n# –í—ã–≤–æ–¥—ã –∏–∑ Run #2:\n# ‚úÖ 100K steps = +187% (vs +118% baseline) - –†–ê–ë–û–¢–ê–ï–¢\n# ‚úÖ High entropy 0.05 = –ª—É—á—à–∏–π Sharpe 2.75 - –†–ê–ë–û–¢–ê–ï–¢\n# ‚ùå Low LR 1e-4 = —Ö—É–∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã - –ù–ï –†–ê–ë–û–¢–ê–ï–¢\n\nEXPERIMENTS = {\n    # ===========================================\n    # BASELINES (–ª—É—á—à–∏–µ –∏–∑ Run #2)\n    # ===========================================\n    \"norm_100k\": {\n        \"name\": \"100K steps (Run #2 best return)\",\n        \"reward_type\": \"simple_pnl\",\n        \"normalize_obs\": True,\n        \"entropy_coef\": 0.01,\n        \"transaction_penalty\": 0.0,\n        \"sharpe_window\": 20,\n        \"action_repeat_penalty\": 0.0,\n        \"learning_rate\": 3e-4,\n        \"timesteps\": 100000,\n    },\n\n    \"norm_high_ent\": {\n        \"name\": \"High Entropy (Run #2 best Sharpe)\",\n        \"reward_type\": \"simple_pnl\",\n        \"normalize_obs\": True,\n        \"entropy_coef\": 0.05,\n        \"transaction_penalty\": 0.0,\n        \"sharpe_window\": 20,\n        \"action_repeat_penalty\": 0.0,\n        \"learning_rate\": 3e-4,\n        \"timesteps\": 50000,\n    },\n\n    # ===========================================\n    # RUN #3 NEW EXPERIMENTS\n    # ===========================================\n\n    # –ì–∏–ø–æ—Ç–µ–∑–∞ 1: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –ª—É—á—à–∏—Ö (100K + high entropy)\n    \"combo_best\": {\n        \"name\": \"100K + High Entropy (combo)\",\n        \"reward_type\": \"simple_pnl\",\n        \"normalize_obs\": True,\n        \"entropy_coef\": 0.05,  # Best Sharpe config\n        \"transaction_penalty\": 0.0,\n        \"sharpe_window\": 20,\n        \"action_repeat_penalty\": 0.0,\n        \"learning_rate\": 3e-4,\n        \"timesteps\": 100000,  # Best return config\n    },\n\n    # –ì–∏–ø–æ—Ç–µ–∑–∞ 2: –µ—â–µ –±–æ–ª—å—à–µ timesteps\n    \"steps_200k\": {\n        \"name\": \"200K steps\",\n        \"reward_type\": \"simple_pnl\",\n        \"normalize_obs\": True,\n        \"entropy_coef\": 0.01,\n        \"transaction_penalty\": 0.0,\n        \"sharpe_window\": 20,\n        \"action_repeat_penalty\": 0.0,\n        \"learning_rate\": 3e-4,\n        \"timesteps\": 200000,  # 2x –æ—Ç –ª—É—á—à–µ–≥–æ\n    },\n\n    # –ì–∏–ø–æ—Ç–µ–∑–∞ 3: –≤—ã—à–µ LR (—Ä–∞–∑ –Ω–∏–∑–∫–∏–π –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)\n    \"high_lr\": {\n        \"name\": \"High LR (5e-4)\",\n        \"reward_type\": \"simple_pnl\",\n        \"normalize_obs\": True,\n        \"entropy_coef\": 0.01,\n        \"transaction_penalty\": 0.0,\n        \"sharpe_window\": 20,\n        \"action_repeat_penalty\": 0.0,\n        \"learning_rate\": 5e-4,  # –í—ã—à–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ\n        \"timesteps\": 50000,\n    },\n\n    # –ì–∏–ø–æ—Ç–µ–∑–∞ 4: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è\n    \"steps_200k_ent\": {\n        \"name\": \"200K + High Entropy (max)\",\n        \"reward_type\": \"simple_pnl\",\n        \"normalize_obs\": True,\n        \"entropy_coef\": 0.05,  # High entropy\n        \"transaction_penalty\": 0.0,\n        \"sharpe_window\": 20,\n        \"action_repeat_penalty\": 0.0,\n        \"learning_rate\": 3e-4,\n        \"timesteps\": 200000,  # Max timesteps\n    },\n}\n\nprint(\"‚úÖ Experiment configurations loaded (Run #3 - Scaling Up)\")\nprint(f\"Total experiments: {len(EXPERIMENTS)}\")\nprint(\"\\nüìã Experiments:\")\nfor key, config in EXPERIMENTS.items():\n    lr = config.get('learning_rate', 3e-4)\n    ts = config.get('timesteps', 50000)\n    print(f\"  - {config['name']}\")\n    print(f\"      LR: {lr}, Steps: {ts//1000}K, Ent: {config['entropy_coef']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## SECTION 3: Data Collection\n\nGenerate synthetic market data and news sentiment for testing.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# SECTION 3: Data Collection (with Caching)\n# ============================================\n# –î–∞–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è 1 —Ä–∞–∑ –∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ –∫–µ—à\n# –ü—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—É—Å–∫–∞—Ö –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –∏–∑ –∫–µ—à–∞ (~–º–≥–Ω–æ–≤–µ–Ω–Ω–æ)\n\ndef generate_synthetic_market_data(n_days=365):\n    \"\"\"\n    Generate synthetic OHLCV data with technical indicators.\n    \"\"\"\n    dates = pd.date_range(end=datetime.now(), periods=n_days, freq='D')\n    \n    # Simulate price with random walk + trend\n    np.random.seed(42)\n    returns = np.random.normal(0.0005, 0.02, n_days)  # Mean daily return ~0.05%, volatility 2%\n    prices = 100 * np.exp(np.cumsum(returns))\n    \n    # Generate OHLCV\n    df = pd.DataFrame({\n        'timestamp': dates,\n        'open': prices * np.random.uniform(0.98, 1.0, n_days),\n        'high': prices * np.random.uniform(1.0, 1.02, n_days),\n        'low': prices * np.random.uniform(0.97, 1.0, n_days),\n        'close': prices,\n        'volume': np.random.uniform(1e6, 5e6, n_days)\n    })\n    \n    # Technical indicators\n    df['returns_1d'] = df['close'].pct_change()\n    df['returns_7d'] = df['close'].pct_change(7)\n    \n    # RSI\n    delta = df['close'].diff()\n    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n    rs = gain / loss\n    df['rsi_14'] = 100 - (100 / (1 + rs))\n    \n    # MACD\n    ema_12 = df['close'].ewm(span=12).mean()\n    ema_26 = df['close'].ewm(span=26).mean()\n    df['macd'] = ema_12 - ema_26\n    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n    \n    # Bollinger Bands\n    sma_20 = df['close'].rolling(window=20).mean()\n    std_20 = df['close'].rolling(window=20).std()\n    df['bollinger_upper'] = sma_20 + (std_20 * 2)\n    df['bollinger_lower'] = sma_20 - (std_20 * 2)\n    \n    # ATR (Average True Range)\n    high_low = df['high'] - df['low']\n    high_close = np.abs(df['high'] - df['close'].shift())\n    low_close = np.abs(df['low'] - df['close'].shift())\n    ranges = pd.concat([high_low, high_close, low_close], axis=1)\n    true_range = np.max(ranges, axis=1)\n    df['atr_14'] = true_range.rolling(14).mean()\n    \n    # Volume ratio\n    df['volume_ratio'] = df['volume'] / df['volume'].rolling(20).mean()\n    \n    df.fillna(0, inplace=True)\n    return df\n\ndef generate_synthetic_news_data(n_days=365):\n    \"\"\"\n    Generate synthetic news sentiment data.\n    \"\"\"\n    dates = pd.date_range(end=datetime.now(), periods=n_days, freq='D')\n    \n    np.random.seed(43)\n    \n    # Sentiment follows a random walk between -1 and 1\n    sentiment_base = np.cumsum(np.random.normal(0, 0.1, n_days))\n    sentiment_base = np.clip(sentiment_base, -3, 3) / 3  # Normalize to [-1, 1]\n    \n    df = pd.DataFrame({\n        'timestamp': dates,\n        'sentiment_1h': sentiment_base + np.random.normal(0, 0.1, n_days),\n        'sentiment_24h': sentiment_base,\n        'sentiment_7d': pd.Series(sentiment_base).rolling(7).mean().fillna(0).values,\n        'sentiment_trend': pd.Series(sentiment_base).diff().fillna(0).values,\n        'news_volume': np.random.poisson(20, n_days),\n        'news_velocity': np.random.uniform(0.5, 2.0, n_days)\n    })\n    \n    # Clip sentiment to [-1, 1]\n    for col in ['sentiment_1h', 'sentiment_24h', 'sentiment_7d', 'sentiment_trend']:\n        df[col] = np.clip(df[col], -1, 1)\n    \n    return df\n\n\ndef get_or_generate_data(n_days=500, force_regenerate=False):\n    \"\"\"\n    Load data from cache or generate new data.\n    Set force_regenerate=True to regenerate even if cache exists.\n    \"\"\"\n    if os.path.exists(DATA_CACHE_PATH) and not force_regenerate:\n        print(\"üì¶ Loading cached data...\")\n        data = np.load(DATA_CACHE_PATH, allow_pickle=True)\n        market_data = pd.DataFrame(data['market'].item())\n        news_data = pd.DataFrame(data['news'].item())\n        print(f\"‚úÖ Loaded from cache: {DATA_CACHE_PATH}\")\n    else:\n        print(\"üîÑ Generating new synthetic data...\")\n        market_data = generate_synthetic_market_data(n_days=n_days)\n        news_data = generate_synthetic_news_data(n_days=n_days)\n        \n        # Save to cache\n        np.savez(DATA_CACHE_PATH, \n                 market=market_data.to_dict(), \n                 news=news_data.to_dict())\n        print(f\"üíæ Saved to cache: {DATA_CACHE_PATH}\")\n    \n    return market_data, news_data\n\n\n# Load or generate data\nmarket_data, news_data = get_or_generate_data(n_days=500)\n\nprint(f\"\\n‚úÖ Data ready\")\nprint(f\"Market data shape: {market_data.shape}\")\nprint(f\"News data shape: {news_data.shape}\")\nprint(f\"\\nMarket data sample:\")\nprint(market_data.head())\nprint(f\"\\nNews data sample:\")\nprint(news_data.head())\n\n# Hint: To regenerate data, run:\n# market_data, news_data = get_or_generate_data(n_days=500, force_regenerate=True)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## SECTION 4: Environment\n\nCustom Gymnasium trading environment with configurable rewards and normalization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class TradingEnv(gym.Env):\n    \"\"\"\n    Custom trading environment compatible with Stable Baselines3.\n    \n    Observation Space:\n        - market: 15 technical indicators\n        - news: 6 sentiment features\n        - portfolio: 5 position metrics\n    \n    Action Space:\n        - Discrete(7): HOLD, BUY_25%, BUY_50%, BUY_100%, SELL_25%, SELL_50%, SELL_100%\n    \"\"\"\n    \n    def __init__(self, market_data, news_data, config=None, initial_balance=10000, commission=0.001):\n        super(TradingEnv, self).__init__()\n        \n        self.market_data = market_data.reset_index(drop=True)\n        self.news_data = news_data.reset_index(drop=True)\n        self.initial_balance = initial_balance\n        self.commission = commission\n        \n        # Experiment configuration\n        if config is None:\n            config = EXPERIMENTS[\"baseline\"]\n        self.config = config\n        \n        # Action space: 0=HOLD, 1-3=BUY, 4-6=SELL\n        self.action_space = spaces.Discrete(7)\n        \n        # Observation space\n        self.observation_space = spaces.Dict({\n            'market': spaces.Box(low=-np.inf, high=np.inf, shape=(15,), dtype=np.float32),\n            'news': spaces.Box(low=-1, high=1, shape=(6,), dtype=np.float32),\n            'portfolio': spaces.Box(low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32)\n        })\n        \n        # Normalization statistics (running mean/std)\n        self.obs_mean = None\n        self.obs_std = None\n        self.obs_count = 0\n        \n        self.reset()\n    \n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        \n        self.current_step = 50  # Start after warm-up period for indicators\n        self.balance = self.initial_balance\n        self.shares_held = 0\n        self.total_value = self.initial_balance\n        self.trades = []\n        self.portfolio_values = [self.initial_balance]\n        self.last_action = 0\n        \n        # For Sharpe-based reward\n        self.recent_returns = []\n        \n        return self._get_observation(), {}\n    \n    def _normalize_observation(self, obs):\n        \"\"\"Apply observation normalization if enabled.\"\"\"\n        if not self.config.get(\"normalize_obs\", False):\n            return obs\n        \n        # Initialize normalization statistics\n        if self.obs_mean is None:\n            self.obs_mean = {k: np.zeros_like(v) for k, v in obs.items()}\n            self.obs_std = {k: np.ones_like(v) for k, v in obs.items()}\n        \n        # Update running statistics (Welford's online algorithm)\n        self.obs_count += 1\n        normalized_obs = {}\n        \n        for key in obs.keys():\n            delta = obs[key] - self.obs_mean[key]\n            self.obs_mean[key] += delta / self.obs_count\n            delta2 = obs[key] - self.obs_mean[key]\n            self.obs_std[key] = np.sqrt((self.obs_std[key]**2 * (self.obs_count - 1) + delta * delta2) / self.obs_count + 1e-8)\n            \n            # Normalize\n            normalized_obs[key] = (obs[key] - self.obs_mean[key]) / (self.obs_std[key] + 1e-8)\n            normalized_obs[key] = np.clip(normalized_obs[key], -10, 10)  # Clip extreme values\n        \n        return normalized_obs\n    \n    def _get_observation(self):\n        \"\"\"Get current observation.\"\"\"\n        row = self.market_data.iloc[self.current_step]\n        news_row = self.news_data.iloc[self.current_step]\n        \n        # Market features (15)\n        market_features = np.array([\n            row['close'] / 100,  # Normalized price\n            row['returns_1d'],\n            row['returns_7d'],\n            row['rsi_14'] / 100,\n            row['macd'] / row['close'] if row['close'] > 0 else 0,\n            row['macd_signal'] / row['close'] if row['close'] > 0 else 0,\n            (row['close'] - row['bollinger_lower']) / (row['bollinger_upper'] - row['bollinger_lower']) if row['bollinger_upper'] != row['bollinger_lower'] else 0.5,\n            row['atr_14'] / row['close'] if row['close'] > 0 else 0,\n            row['volume_ratio'],\n            row['volume'] / 1e6,  # Normalized volume\n            (row['high'] - row['low']) / row['close'] if row['close'] > 0 else 0,\n            (row['close'] - row['open']) / row['open'] if row['open'] > 0 else 0,\n            row['high'] / row['close'] if row['close'] > 0 else 1,\n            row['low'] / row['close'] if row['close'] > 0 else 1,\n            row['volume'] / row['volume'] if self.current_step == 0 else row['volume'] / self.market_data.iloc[self.current_step-1]['volume']\n        ], dtype=np.float32)\n        \n        # News features (6)\n        news_features = np.array([\n            news_row['sentiment_1h'],\n            news_row['sentiment_24h'],\n            news_row['sentiment_7d'],\n            news_row['sentiment_trend'],\n            news_row['news_volume'] / 50,  # Normalized\n            news_row['news_velocity']\n        ], dtype=np.float32)\n        \n        # Portfolio features (5)\n        current_price = row['close']\n        portfolio_value = self.balance + self.shares_held * current_price\n        \n        portfolio_features = np.array([\n            self.balance / self.initial_balance,  # Cash ratio\n            self.shares_held * current_price / self.initial_balance if self.initial_balance > 0 else 0,  # Position ratio\n            portfolio_value / self.initial_balance - 1,  # Return\n            self.shares_held / 100 if self.shares_held > 0 else 0,  # Normalized shares\n            len(self.trades) / 100  # Normalized trade count\n        ], dtype=np.float32)\n        \n        obs = {\n            'market': market_features,\n            'news': news_features,\n            'portfolio': portfolio_features\n        }\n        \n        return self._normalize_observation(obs)\n    \n    def _calculate_reward(self, portfolio_value, action):\n        \"\"\"Calculate reward based on configuration.\"\"\"\n        reward_type = self.config.get(\"reward_type\", \"simple_pnl\")\n        \n        if reward_type == \"simple_pnl\":\n            # Simple P&L reward\n            reward = (portfolio_value - self.total_value) / self.total_value\n        \n        elif reward_type == \"sharpe_based\":\n            # Sharpe-based reward (risk-adjusted returns)\n            portfolio_return = (portfolio_value - self.total_value) / self.total_value\n            self.recent_returns.append(portfolio_return)\n            \n            # Keep only recent window\n            window = self.config.get(\"sharpe_window\", 20)\n            if len(self.recent_returns) > window:\n                self.recent_returns.pop(0)\n            \n            # Calculate Sharpe-like reward\n            if len(self.recent_returns) >= 2:\n                mean_return = np.mean(self.recent_returns)\n                std_return = np.std(self.recent_returns)\n                sharpe = mean_return / (std_return + 1e-9)\n                reward = sharpe\n            else:\n                reward = portfolio_return\n        \n        else:\n            reward = 0\n        \n        # Apply transaction penalty\n        transaction_penalty = self.config.get(\"transaction_penalty\", 0.0)\n        if action != 0:  # Not HOLD\n            reward -= transaction_penalty\n        \n        # Apply action repeat penalty (discourage same action repeatedly)\n        action_repeat_penalty = self.config.get(\"action_repeat_penalty\", 0.0)\n        if action == self.last_action and action != 0:\n            reward -= action_repeat_penalty\n        \n        return reward\n    \n    def step(self, action):\n        \"\"\"Execute one time step.\"\"\"\n        current_price = self.market_data.iloc[self.current_step]['close']\n        \n        # Execute action\n        if action == 0:  # HOLD\n            pass\n        elif action in [1, 2, 3]:  # BUY\n            buy_pct = [0.25, 0.5, 1.0][action - 1]\n            amount_to_invest = self.balance * buy_pct\n            shares_to_buy = (amount_to_invest / current_price) * (1 - self.commission)\n            \n            if shares_to_buy > 0:\n                self.shares_held += shares_to_buy\n                self.balance -= amount_to_invest\n                self.trades.append({\n                    'step': self.current_step,\n                    'action': 'BUY',\n                    'shares': shares_to_buy,\n                    'price': current_price\n                })\n        \n        elif action in [4, 5, 6]:  # SELL\n            sell_pct = [0.25, 0.5, 1.0][action - 4]\n            shares_to_sell = self.shares_held * sell_pct\n            \n            if shares_to_sell > 0:\n                self.balance += shares_to_sell * current_price * (1 - self.commission)\n                self.shares_held -= shares_to_sell\n                self.trades.append({\n                    'step': self.current_step,\n                    'action': 'SELL',\n                    'shares': shares_to_sell,\n                    'price': current_price\n                })\n        \n        # Calculate portfolio value\n        portfolio_value = self.balance + self.shares_held * current_price\n        self.portfolio_values.append(portfolio_value)\n        \n        # Calculate reward\n        reward = self._calculate_reward(portfolio_value, action)\n        self.total_value = portfolio_value\n        self.last_action = action\n        \n        # Move to next step\n        self.current_step += 1\n        \n        # Check if episode is done\n        done = self.current_step >= len(self.market_data) - 1\n        truncated = False\n        \n        return self._get_observation(), reward, done, truncated, {}\n    \n    def render(self, mode='human'):\n        \"\"\"Render the environment (optional).\"\"\"\n        current_price = self.market_data.iloc[self.current_step]['close']\n        portfolio_value = self.balance + self.shares_held * current_price\n        profit = ((portfolio_value / self.initial_balance) - 1) * 100\n        \n        print(f\"Step: {self.current_step} | Price: ${current_price:.2f} | \"\n              f\"Balance: ${self.balance:.2f} | Shares: {self.shares_held:.2f} | \"\n              f\"Portfolio: ${portfolio_value:.2f} | Profit: {profit:.2f}%\")\n\nprint(\"‚úÖ Environment class defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## SECTION 5: Training\n\nTrain multiple experiments and compare results."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# SECTION 5: Training (with Model Saving)\n# ============================================\n# –û–±—É—á–∞–µ—Ç –≤—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∏ –°–û–•–†–ê–ù–Ø–ï–¢ –º–æ–¥–µ–ª–∏\n# –ò—Å–ø–æ–ª—å–∑—É–µ—Ç learning_rate –∏ timesteps –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∫–∞–∂–¥–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n# ‚ö†Ô∏è Run #3: 200K steps —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∑–∞–π–º—É—Ç ~10 –º–∏–Ω –∫–∞–∂–¥—ã–π\n\nclass ProgressCallback(BaseCallback):\n    \"\"\"Custom callback for logging training progress.\"\"\"\n    def __init__(self, check_freq, verbose=1):\n        super(ProgressCallback, self).__init__(verbose)\n        self.check_freq = check_freq\n        self.episode_rewards = []\n        self.episode_lengths = []\n    \n    def _on_step(self):\n        if self.n_calls % self.check_freq == 0:\n            if len(self.model.ep_info_buffer) > 0:\n                mean_reward = np.mean([ep_info['r'] for ep_info in self.model.ep_info_buffer])\n                mean_length = np.mean([ep_info['l'] for ep_info in self.model.ep_info_buffer])\n                self.episode_rewards.append(mean_reward)\n                self.episode_lengths.append(mean_length)\n                if self.verbose > 0:\n                    print(f\"  Step: {self.n_calls:6d} | Mean reward: {mean_reward:8.4f} | Mean ep length: {mean_length:.1f}\")\n        return True\n\n\n# ============================================\n# Model Save/Load Functions\n# ============================================\n\ndef save_model(model, experiment_name):\n    \"\"\"Save trained model to disk.\"\"\"\n    os.makedirs(MODELS_DIR, exist_ok=True)\n    path = f'{MODELS_DIR}/{experiment_name}.zip'\n    model.save(path)\n    print(f\"üíæ Model saved: {path}\")\n    return path\n\ndef load_model(experiment_name, env=None):\n    \"\"\"Load model from disk. Returns None if not found.\"\"\"\n    path = f'{MODELS_DIR}/{experiment_name}.zip'\n    if os.path.exists(path):\n        print(f\"üìÇ Loading model: {path}\")\n        return PPO.load(path, env=env)\n    return None\n\ndef list_saved_models():\n    \"\"\"List all saved models.\"\"\"\n    if not os.path.exists(MODELS_DIR):\n        return []\n    return [f.replace('.zip', '') for f in os.listdir(MODELS_DIR) if f.endswith('.zip')]\n\ndef save_results(results_dict):\n    \"\"\"Save experiment results to JSON.\"\"\"\n    serializable = {}\n    for exp_key, results in results_dict.items():\n        if 'error' in results:\n            serializable[exp_key] = results\n        else:\n            serializable[exp_key] = {\n                k: v if not isinstance(v, (np.ndarray, list)) or k != 'portfolio_values' \n                else [float(x) for x in v]\n                for k, v in results.items()\n            }\n    with open(RESULTS_PATH, 'w') as f:\n        json.dump(serializable, f, indent=2, default=str)\n    print(f\"üíæ Results saved: {RESULTS_PATH}\")\n\ndef load_results():\n    \"\"\"Load experiment results from JSON.\"\"\"\n    if os.path.exists(RESULTS_PATH):\n        with open(RESULTS_PATH, 'r') as f:\n            return json.load(f)\n    return None\n\n\ndef evaluate_agent(model, market_data, news_data, config):\n    \"\"\"Evaluate a trained agent and return metrics.\"\"\"\n    eval_env = TradingEnv(market_data, news_data, config=config, initial_balance=10000)\n    obs, info = eval_env.reset()\n    done = False\n    \n    actions_taken = []\n    rewards_list = []\n    \n    while not done:\n        action, _ = model.predict(obs, deterministic=True)\n        actions_taken.append(int(action))\n        obs, reward, done, truncated, info = eval_env.step(action)\n        rewards_list.append(reward)\n        done = done or truncated\n    \n    final_price = eval_env.market_data.iloc[eval_env.current_step - 1]['close']\n    final_value = eval_env.balance + eval_env.shares_held * final_price\n    total_return = (final_value / eval_env.initial_balance - 1) * 100\n    \n    initial_price = eval_env.market_data.iloc[50]['close']\n    buy_hold_return = ((final_price / initial_price) - 1) * 100\n    \n    returns_array = np.array(eval_env.portfolio_values[1:]) / np.array(eval_env.portfolio_values[:-1]) - 1\n    sharpe = np.mean(returns_array) / (np.std(returns_array) + 1e-9) * np.sqrt(252)\n    \n    portfolio_values = np.array(eval_env.portfolio_values)\n    running_max = np.maximum.accumulate(portfolio_values)\n    drawdown = (portfolio_values - running_max) / running_max\n    max_drawdown = np.min(drawdown) * 100\n    \n    winning_trades = sum(1 for r in rewards_list if r > 0)\n    win_rate = (winning_trades / len(rewards_list) * 100) if len(rewards_list) > 0 else 0\n    \n    action_names = ['HOLD', 'BUY_25%', 'BUY_50%', 'BUY_100%', 'SELL_25%', 'SELL_50%', 'SELL_100%']\n    action_dist = {name: actions_taken.count(i) / len(actions_taken) * 100 for i, name in enumerate(action_names)}\n    \n    return {\n        'final_value': final_value,\n        'total_return': total_return,\n        'buy_hold_return': buy_hold_return,\n        'outperformance': total_return - buy_hold_return,\n        'sharpe': sharpe,\n        'max_drawdown': max_drawdown,\n        'win_rate': win_rate,\n        'num_trades': len(eval_env.trades),\n        'action_dist': action_dist,\n        'portfolio_values': eval_env.portfolio_values\n    }\n\n\n# ============================================\n# RUN TRAINING\n# ============================================\n\nexperiment_results = {}\ntotal_start_time = time.time()\n\nprint(\"=\"*80)\nprint(\"STARTING MULTI-EXPERIMENT TRAINING (Run #3 - Scaling Up)\")\nprint(\"=\"*80)\nprint(f\"üìÅ Models will be saved to: {MODELS_DIR}\")\nprint(f\"üìä Total experiments: {len(EXPERIMENTS)}\")\nprint(f\"‚è±Ô∏è Estimated time: ~15-20 min (200K steps experiments)\")\n\nfor exp_key, exp_config in EXPERIMENTS.items():\n    try:\n        # Get config-specific hyperparameters\n        learning_rate = exp_config.get('learning_rate', 3e-4)\n        total_timesteps = exp_config.get('timesteps', 50000)\n        entropy_coef = exp_config.get('entropy_coef', 0.01)\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"EXPERIMENT: {exp_config['name']}\")\n        print(f\"{'='*80}\")\n        print(f\"Hyperparameters:\")\n        print(f\"  - learning_rate: {learning_rate}\")\n        print(f\"  - timesteps: {total_timesteps}\")\n        print(f\"  - entropy_coef: {entropy_coef}\")\n        print(f\"  - normalize_obs: {exp_config.get('normalize_obs', False)}\")\n        print(f\"  - transaction_penalty: {exp_config.get('transaction_penalty', 0.0)}\")\n        print()\n        \n        # Create environment\n        train_env = TradingEnv(market_data, news_data, config=exp_config)\n        \n        # Initialize PPO with config-specific parameters\n        model = PPO(\n            \"MultiInputPolicy\",\n            train_env,\n            learning_rate=learning_rate,  # From config!\n            n_steps=2048,\n            batch_size=64,\n            n_epochs=10,\n            gamma=0.99,\n            gae_lambda=0.95,\n            clip_range=0.2,\n            ent_coef=entropy_coef,  # From config!\n            verbose=0\n        )\n        \n        print(f\"Starting training ({total_timesteps//1000}K timesteps)...\")\n        \n        # Calculate epochs based on timesteps\n        timesteps_per_epoch = 10000\n        epochs = total_timesteps // timesteps_per_epoch\n        \n        callback = ProgressCallback(check_freq=10000, verbose=1)\n        start_time = time.time()\n        \n        for epoch in range(epochs):\n            print(f\"\\n  Epoch {epoch + 1}/{epochs}:\")\n            model.learn(\n                total_timesteps=timesteps_per_epoch,\n                callback=callback,\n                reset_num_timesteps=False\n            )\n        \n        training_time = time.time() - start_time\n        print(f\"\\n‚úÖ Training complete in {training_time:.2f}s\")\n        \n        # Save model\n        save_model(model, exp_key)\n        \n        # Evaluate\n        print(f\"Evaluating...\")\n        results = evaluate_agent(model, market_data, news_data, exp_config)\n        results['training_time'] = training_time\n        results['config'] = exp_config\n        \n        experiment_results[exp_key] = results\n        \n        print(f\"‚úÖ {exp_config['name']} complete!\")\n        print(f\"   Return: {results['total_return']:+.2f}% | Sharpe: {results['sharpe']:.2f} | Drawdown: {results['max_drawdown']:.2f}%\")\n        \n    except Exception as e:\n        print(f\"\\n‚ùå ERROR in experiment {exp_key}:\")\n        print(f\"Error type: {type(e).__name__}\")\n        print(f\"Error message: {str(e)}\")\n        traceback.print_exc()\n        experiment_results[exp_key] = {'error': str(e), 'config': exp_config}\n\n# Save results\nsave_results(experiment_results)\n\ntotal_time = time.time() - total_start_time\nprint(f\"\\n{'='*80}\")\nprint(f\"ALL EXPERIMENTS COMPLETE\")\nprint(f\"Total time: {total_time:.2f}s ({total_time/60:.1f} minutes)\")\nprint(f\"{'='*80}\")\n\nsaved = list_saved_models()\nprint(f\"\\nüíæ Saved models ({len(saved)}):\")\nfor m in saved:\n    print(f\"  - {m}\")"
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# SECTION 5b: Quick Evaluate (Skip Training)\n# ============================================\n# –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏—Ö\n# –ù–ï –ü–ï–†–ï–û–ë–£–ß–ê–ï–¢ - —ç–∫–æ–Ω–æ–º–∏—Ç ~5 –º–∏–Ω—É—Ç!\n\nprint(\"=\"*80)\nprint(\"QUICK EVALUATE - Loading saved models\")\nprint(\"=\"*80)\n\n# Check for saved models\nsaved_models = list_saved_models()\nprint(f\"\\nüìÇ Found {len(saved_models)} saved models: {saved_models}\")\n\nif len(saved_models) == 0:\n    print(\"\\n‚ö†Ô∏è No saved models found!\")\n    print(\"Run Section 5 (Training) first to train and save models.\")\nelse:\n    experiment_results = {}\n    \n    for exp_key, exp_config in EXPERIMENTS.items():\n        print(f\"\\n{'='*60}\")\n        print(f\"Loading: {exp_config['name']}\")\n        \n        model = load_model(exp_key)\n        \n        if model is None:\n            print(f\"  ‚ö†Ô∏è Model not found for {exp_key}, skipping...\")\n            continue\n        \n        # Evaluate\n        print(f\"  Evaluating...\")\n        results = evaluate_agent(model, market_data, news_data, exp_config)\n        results['training_time'] = 0  # Not trained this session\n        results['config'] = exp_config\n        \n        experiment_results[exp_key] = results\n        \n        print(f\"  ‚úÖ Return: {results['total_return']:+.2f}% | Sharpe: {results['sharpe']:.2f} | Drawdown: {results['max_drawdown']:.2f}%\")\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"‚úÖ Quick Evaluate complete! Loaded {len(experiment_results)} models.\")\n    print(\"Now run Section 6-7 to see detailed results.\")\n    print(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## SECTION 5b: Quick Evaluate (Skip Training)\n\n**–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç—É —è—á–µ–π–∫—É —á—Ç–æ–±—ã –∑–∞–≥—Ä—É–∑–∏—Ç—å —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ë–ï–ó –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.**\n\n–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:\n- –ü–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ Run All (–º–æ–¥–µ–ª–∏ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã)\n- –ö–æ–≥–¥–∞ —Ö–æ—á–µ—à—å –ø—Ä–æ—Å—Ç–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n- –ö–æ–≥–¥–∞ –∏–∑–º–µ–Ω–∏–ª —Ç–æ–ª—å–∫–æ –∫–æ–Ω—Ñ–∏–≥–∏ –≤—ã–≤–æ–¥–∞ (Section 6-7)\n\n‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç: Section 1-4 –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∑–∞–ø—É—â–µ–Ω—ã",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## SECTION 6: Results Comparison\n\nDisplay comparison table of all experiments with clear markers for Claude."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"CLAUDE_RESULTS_START\")\nprint(\"=\"*80)\n\nprint(\"\\nüìä EXPERIMENT COMPARISON TABLE\")\nprint(\"=\"*80)\n\n# Create comparison table\ncomparison_data = []\nfor exp_key, results in experiment_results.items():\n    if 'error' in results:\n        comparison_data.append({\n            'Experiment': results['config']['name'],\n            'Status': 'ERROR',\n            'Return': 'N/A',\n            'Sharpe': 'N/A',\n            'Drawdown': 'N/A',\n            'Win Rate': 'N/A',\n            'Trades': 'N/A',\n            'Time': 'N/A'\n        })\n    else:\n        comparison_data.append({\n            'Experiment': results['config']['name'],\n            'Status': '‚úÖ',\n            'Return': f\"{results['total_return']:+.2f}%\",\n            'Sharpe': f\"{results['sharpe']:.2f}\",\n            'Drawdown': f\"{results['max_drawdown']:.2f}%\",\n            'Win Rate': f\"{results['win_rate']:.2f}%\",\n            'Trades': str(results['num_trades']),\n            'Time': f\"{results['training_time']:.1f}s\"\n        })\n\n# Print table header\nheaders = ['Experiment', 'Status', 'Return', 'Sharpe', 'Drawdown', 'Win Rate', 'Trades', 'Time']\ncol_widths = [35, 8, 12, 10, 12, 12, 8, 10]\n\nheader_row = \"\"\nfor header, width in zip(headers, col_widths):\n    header_row += f\"{header:<{width}}\"\nprint(header_row)\nprint(\"-\" * 80)\n\n# Print table rows\nfor row in comparison_data:\n    row_str = \"\"\n    for header, width in zip(headers, col_widths):\n        row_str += f\"{row[header]:<{width}}\"\n    print(row_str)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìà DETAILED RESULTS BY EXPERIMENT\")\nprint(\"=\"*80)\n\nfor exp_key, results in experiment_results.items():\n    if 'error' in results:\n        print(f\"\\n‚ùå {results['config']['name']}\")\n        print(f\"   Error: {results['error']}\")\n        continue\n    \n    print(f\"\\n{results['config']['name']}\")\n    print(\"-\" * 80)\n    print(f\"Performance Metrics:\")\n    print(f\"  Total Return:          {results['total_return']:+.2f}%\")\n    print(f\"  Buy & Hold Return:     {results['buy_hold_return']:+.2f}%\")\n    print(f\"  Outperformance:        {results['outperformance']:+.2f}%\")\n    print(f\"  Sharpe Ratio:          {results['sharpe']:.2f}\")\n    print(f\"  Max Drawdown:          {results['max_drawdown']:.2f}%\")\n    print(f\"  Win Rate:              {results['win_rate']:.2f}%\")\n    print(f\"  Total Trades:          {results['num_trades']}\")\n    print(f\"  Training Time:         {results['training_time']:.2f}s\")\n    \n    print(f\"\\nAction Distribution:\")\n    for action_name, pct in results['action_dist'].items():\n        bar = \"‚ñà\" * int(pct / 2)\n        print(f\"  {action_name:12} {pct:5.1f}% {bar}\")\n    \n    print(f\"\\nConfiguration:\")\n    for key, value in results['config'].items():\n        if key != 'name':\n            print(f\"  {key:25} {value}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üèÜ BEST PERFORMERS\")\nprint(\"=\"*80)\n\n# Find best performers\nvalid_results = {k: v for k, v in experiment_results.items() if 'error' not in v}\n\nif valid_results:\n    best_return = max(valid_results.items(), key=lambda x: x[1]['total_return'])\n    best_sharpe = max(valid_results.items(), key=lambda x: x[1]['sharpe'])\n    best_drawdown = max(valid_results.items(), key=lambda x: -x[1]['max_drawdown'])  # Less negative is better\n    best_winrate = max(valid_results.items(), key=lambda x: x[1]['win_rate'])\n    \n    print(f\"Best Return:       {best_return[1]['config']['name']:40} {best_return[1]['total_return']:+.2f}%\")\n    print(f\"Best Sharpe Ratio: {best_sharpe[1]['config']['name']:40} {best_sharpe[1]['sharpe']:.2f}\")\n    print(f\"Best Drawdown:     {best_drawdown[1]['config']['name']:40} {best_drawdown[1]['max_drawdown']:.2f}%\")\n    print(f\"Best Win Rate:     {best_winrate[1]['config']['name']:40} {best_winrate[1]['win_rate']:.2f}%\")\n    \n    print(f\"\\nüìå RECOMMENDATION\")\n    print(\"-\" * 80)\n    \n    # Calculate composite score\n    scores = {}\n    for exp_key, results in valid_results.items():\n        # Normalize metrics (higher is better)\n        score = (\n            results['total_return'] / 100 * 0.3 +  # 30% weight on returns\n            results['sharpe'] / 2 * 0.3 +           # 30% weight on Sharpe\n            -results['max_drawdown'] / 20 * 0.2 +   # 20% weight on drawdown\n            results['win_rate'] / 100 * 0.2         # 20% weight on win rate\n        )\n        scores[exp_key] = score\n    \n    best_overall = max(scores.items(), key=lambda x: x[1])\n    best_exp = valid_results[best_overall[0]]\n    \n    print(f\"Best Overall:      {best_exp['config']['name']}\")\n    print(f\"  Composite Score: {best_overall[1]:.4f}\")\n    print(f\"  Total Return:    {best_exp['total_return']:+.2f}%\")\n    print(f\"  Sharpe Ratio:    {best_exp['sharpe']:.2f}\")\n    print(f\"  Max Drawdown:    {best_exp['max_drawdown']:.2f}%\")\n    print(f\"  Win Rate:        {best_exp['win_rate']:.2f}%\")\nelse:\n    print(\"No valid results to compare.\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"CLAUDE_RESULTS_END\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "try:\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    \n    # Filter valid results\n    valid_results = {k: v for k, v in experiment_results.items() if 'error' not in v}\n    \n    if not valid_results:\n        print(\"No valid results to visualize.\")\n    else:\n        # Plot 1: Portfolio value comparison\n        ax1 = axes[0, 0]\n        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n        for idx, (exp_key, results) in enumerate(valid_results.items()):\n            steps = range(len(results['portfolio_values']))\n            ax1.plot(steps, results['portfolio_values'], \n                    label=results['config']['name'], \n                    linewidth=2, \n                    color=colors[idx % len(colors)])\n        \n        ax1.axhline(y=10000, color='gray', linestyle='--', alpha=0.5, label='Initial Balance')\n        ax1.set_title('Portfolio Value Over Time - All Experiments', fontsize=14, fontweight='bold')\n        ax1.set_xlabel('Steps')\n        ax1.set_ylabel('Portfolio Value ($)')\n        ax1.legend(loc='best', fontsize=9)\n        ax1.grid(True, alpha=0.3)\n        \n        # Plot 2: Returns comparison (bar chart)\n        ax2 = axes[0, 1]\n        exp_names = [v['config']['name'][:20] for v in valid_results.values()]\n        returns = [v['total_return'] for v in valid_results.values()]\n        buy_hold = [v['buy_hold_return'] for v in valid_results.values()]\n        \n        x = np.arange(len(exp_names))\n        width = 0.35\n        \n        bars1 = ax2.bar(x - width/2, returns, width, label='Agent Return', color='#2ca02c')\n        bars2 = ax2.bar(x + width/2, buy_hold, width, label='Buy & Hold', color='#d62728')\n        \n        ax2.set_title('Total Return Comparison', fontsize=14, fontweight='bold')\n        ax2.set_ylabel('Return (%)')\n        ax2.set_xticks(x)\n        ax2.set_xticklabels(exp_names, rotation=45, ha='right', fontsize=8)\n        ax2.legend()\n        ax2.grid(True, alpha=0.3, axis='y')\n        ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n        \n        # Add value labels on bars\n        for bars in [bars1, bars2]:\n            for bar in bars:\n                height = bar.get_height()\n                ax2.text(bar.get_x() + bar.get_width()/2., height,\n                        f'{height:.1f}%',\n                        ha='center', va='bottom' if height > 0 else 'top', \n                        fontsize=7)\n        \n        # Plot 3: Sharpe Ratio & Max Drawdown\n        ax3 = axes[1, 0]\n        sharpe_ratios = [v['sharpe'] for v in valid_results.values()]\n        max_drawdowns = [abs(v['max_drawdown']) for v in valid_results.values()]\n        \n        x = np.arange(len(exp_names))\n        \n        ax3_twin = ax3.twinx()\n        \n        bars1 = ax3.bar(x - width/2, sharpe_ratios, width, label='Sharpe Ratio', color='#1f77b4', alpha=0.8)\n        bars2 = ax3_twin.bar(x + width/2, max_drawdowns, width, label='Max Drawdown (abs)', color='#ff7f0e', alpha=0.8)\n        \n        ax3.set_title('Risk-Adjusted Metrics', fontsize=14, fontweight='bold')\n        ax3.set_ylabel('Sharpe Ratio', color='#1f77b4')\n        ax3_twin.set_ylabel('Max Drawdown (%) [abs]', color='#ff7f0e')\n        ax3.set_xticks(x)\n        ax3.set_xticklabels(exp_names, rotation=45, ha='right', fontsize=8)\n        ax3.tick_params(axis='y', labelcolor='#1f77b4')\n        ax3_twin.tick_params(axis='y', labelcolor='#ff7f0e')\n        ax3.grid(True, alpha=0.3, axis='y')\n        \n        # Add legends\n        lines1, labels1 = ax3.get_legend_handles_labels()\n        lines2, labels2 = ax3_twin.get_legend_handles_labels()\n        ax3.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=9)\n        \n        # Plot 4: Win Rate comparison\n        ax4 = axes[1, 1]\n        win_rates = [v['win_rate'] for v in valid_results.values()]\n        \n        bars = ax4.barh(exp_names, win_rates, color=colors[:len(exp_names)])\n        ax4.set_title('Win Rate Comparison', fontsize=14, fontweight='bold')\n        ax4.set_xlabel('Win Rate (%)')\n        ax4.axvline(x=50, color='red', linestyle='--', alpha=0.5, label='50% (Random)')\n        ax4.legend()\n        ax4.grid(True, alpha=0.3, axis='x')\n        \n        # Add value labels\n        for i, (bar, val) in enumerate(zip(bars, win_rates)):\n            ax4.text(val + 1, i, f'{val:.1f}%', va='center', fontsize=9)\n        \n        plt.tight_layout()\n        plt.savefig('experiment_comparison.png', dpi=150, bbox_inches='tight')\n        plt.show()\n        \n        print(\"\\n‚úÖ Visualization complete\")\n        print(\"Plot saved as: experiment_comparison.png\")\n\nexcept Exception as e:\n    print(f\"\\n‚ùå ERROR during visualization:\")\n    print(f\"Error: {str(e)}\")\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "source": "---\n## SECTION 7: Save Results\n\nSave all results to a downloadable text file for Claude to review.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary\n\nThis notebook implements a **multi-experiment RL trading agent** with A/B testing framework:\n\n**What's included:**\n1. **Setup**: Installed dependencies (stable-baselines3, gymnasium, etc.)\n2. **Experiment Configuration**: 4 experiments to compare different approaches\n   - Baseline (Simple PnL reward)\n   - Sharpe-based Reward (risk-adjusted returns + transaction penalties)\n   - Normalized Observations (feature scaling for better learning)\n   - Best Combo (Sharpe + Normalized + higher entropy)\n3. **Data**: Generated synthetic market data (OHLCV + technical indicators) and news sentiment\n4. **Environment**: Custom Gymnasium trading environment with:\n   - Configurable reward functions (simple_pnl, sharpe_based)\n   - Optional observation normalization\n   - Transaction and action repeat penalties\n   - Observation space: market (15), news (6), portfolio (5) features\n   - Action space: 7 discrete actions (HOLD, BUY 25/50/100%, SELL 25/50/100%)\n5. **Training**: Trained PPO agent for each experiment (5 epochs √ó 10K timesteps = 50K total)\n6. **Results**: Comprehensive comparison table with:\n   - Performance metrics (Return, Sharpe, Drawdown, Win Rate)\n   - Detailed action distributions\n   - Best performer identification\n   - Composite scoring for overall recommendation\n7. **Visualization**: Multi-panel comparison plots\n\n**How to use this notebook:**\n1. Open in Google Colab: https://colab.research.google.com/github/AssTrahanec/rl-trading-agent/blob/main/colab_notebooks/rl_training.ipynb\n2. Runtime ‚Üí Run All\n3. Wait for all experiments to complete (~10-15 minutes for 4 experiments)\n4. Copy everything between CLAUDE_RESULTS_START/END markers\n5. Share with Claude for analysis\n\n**Iterative workflow:**\n- Claude analyzes results ‚Üí identifies best approaches\n- Claude updates experiment configurations or adds new experiments\n- Claude commits and pushes to GitHub\n- User refreshes Colab (F5) ‚Üí Run All ‚Üí Copy results\n- Repeat until performance is satisfactory\n\n**Possible next improvements:**\n- Add more experiments (different algorithms: SAC, A2C)\n- Test different hyperparameters (learning rate, entropy coefficient)\n- Implement real market data (yfinance, ccxt)\n- Add FinBERT for real news sentiment analysis\n- Implement walk-forward validation\n- Add more sophisticated reward functions"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}